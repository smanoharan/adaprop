\documentclass[a4paper,12pt]{article} % use larger type; default would be 10pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[parfill]{parskip} % Begin paragraphs with an empty line rather than an indent
\usepackage{apacite} % apa style citations and references
\usepackage{fullpage} % reduce margins
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{url} % for rendering urls (in bibliography)
\usepackage{xspace} % for spaces after macros
\usepackage{setspace}

%% ABBREVIATIONS
\newcommand{\AdaProp}{\texttt{AdaProp}\xspace}
\newcommand{\real}{\mathbb{R}}
\newcommand{\mcl}[1]{\mathcal{#1}}
\newcommand{\power}{\mathbb{P}}

%\linespread{1.3} % 1.3 *is* one-and-a-half spacing

\title{\ \\ \  \\ Adaptive Propositionalisation of Multi-Instance Data Towards Image Classification}
\author{Siva Manoharan, supervised by Eibe Frank}
%\date{} % No date


\begin{document}
\onehalfspacing
\pagenumbering{gobble}  % don't render a page number on the first page

\maketitle 
\ \\ \ \\ \ \\ \ \\ \ \\ \ \\ \ % Some blank lines
\begin{abstract}
Multi-instance learning a type of supervised machine learning 
    where the class labels are attached to bags of instances. 
This is in contrast to single-instance learning 
    where each individual instance is given a class label.
Propositionalisation is the process of 
    converting a multi-instance dataset into a single-instance dataset, 
    allowing standard machine learning algorithms to learn the converted dataset. 
In this project, 
    we examine an adaptive approach to propositionalising multi-instance data, 
    where the single-instance base learner is 
    used to guide the propositionalisation process, 
    resulting in a converted dataset 
    which is more suited to the specific base learner.
In the future, 
    we will also examine the application of multi-instance learning 
    to image classification and 
    evaluate the algorithm developed in this project 
    on existing image classification datasets.
\end{abstract}

\thispagestyle{empty}
\clearpage
\pagenumbering{arabic}  

\section{Introduction} 

Multi-instance (MI) learning is 
    machine learning over multi-instance data, 
    where the instances are grouped together into bags and 
    the learning is performed over the bags 
    rather than the individual instances. 
One approach to handling multi-instance data is propositionalisation, 
    where each bag of instances is converted into 
    a single feature vector. 
This converted dataset can be used with 
    standard single-instance machine learning algorithms 
    such as SVMs and Neural Networks.

This project will explore adaptive propositionalisation, where the single-instance base learner is used to make decisions when propositionalising the dataset. The main propositionalisation approach considered will be the partitioning of the instance space into regions, where each region corresponds to a feature in the converted dataset.

    
An application of multi-instance learning is image classification, where, for example, an image is represented as a bag of segments and the interaction between the segments contributes to the class of the image. In this context, each segment is an instance and thus each image is a bag of instances. We will also examine this application of multi-instance learning.

\section{Background}

Multi-instance learning was introduced by \citeA{Diet97} in the context of drug activity prediction. The multi-instance learning problem was defined as a two-class supervised learning problem where the classification occurs over bags of instances rather than the individual instances. \citeA{Diet97} assumed that each instance had a hidden class label and that a bag belongs to the positive class if and only if at least one of the instances in the bag belongs to the positive class. This assumption is known as the ``standard MI assumption'' and fits the original domain of drug activity prediction well.

Multi-instance learning has also been applied to other domains such as image classification and text classification where the standard MI assumption is less appropriate. Therefore, a number of generalisations to the multi-instance problem have been proposed.
\citeA{Chen2006} proposed a generalised MI assumption in which the label of each bag is determined by the distance of the bag's instances to some hidden target points in instance space.
Similarly, \citeA{Weidmann2003} proposed a hierarchy of MI assumptions where the bag's class is determined by the number of instances of the bag which lay in certain regions of instance space.
This project will use the same generalised MI assumptions as \citeA{Weidmann2003}.

There have been a number of proposed algorithms for dealing with multi-instance data. The original algorithm proposed by \citeA{Diet97} used axis parallel rectangles to identify the regions of the instance space where the positive instances lay. The algorithm was designed to find the regions which contained at least one instance from each positive bag but no instances from the negative class. 
\citeA{Maron98mil} proposed the Diverse Density algorithm, where the aim was to find target points in instance space which are close to at least one instance from each positive bag and far away from all instances in negative bags.

\citeA{Weidmann2003} proposed a two level approach for dealing with multi-instance data subject to the generalised MI assumptions. In the first level, a decision tree was built to partition the instance space. In the second level, the occupancy counts of instances of each bag in each region was used to build a propositionalised dataset, to which a single instance learner was applied.
The algorithm being developed in this project will use a similar process as \citeA{Weidmann2003} in the second level, but we will consider an adaptive approach to partition the instance space.

The problem of image classification fits the multi-instance learning setting well. Since an image may contain multiple objects, a single feature vector may not capture all of the information in the image. As a MI learning problem, an image can be considered to be a bag of instances, where each instance is a segment of interest in the image. There is existing work on applying MI learning to image classification: \citeA{Maron98scene} performed learning over scene images by using fixed size partitions of the image. \citeA{Zhang2002} used K-means to segment the image. We will optimise the algorithm to perform well with MI image classification datasets.

%[http://www.cs.cmu.edu/~juny/MILL/review.htm] 

\section{Method}

Our approach, named \AdaProp, is an adaptive propositionalisation algorithm 
    for multi-instance datasets.
\AdaProp divides the single-instance space
    (the space defined by each (single)-instance in each bag of the MI dataset)
    into regions and then
    propositionalises each bag by computing some summary statistics for the 
    subset of instances of the bag which lie in each region.

\AdaProp divides the instance space into regions
    by repeatedly bisecting the space.
Therefore, \AdaProp consists of three major components: 
    An algorithm for choosing the hyperplane to bisect the space *,
    An algorithm for constructing the propositionalised dataset 
        using each bag and each region, and
    A (single-instance) base learner to guide the above the two algorithms.

\subsection{Definitions}
Formally, we define a multi-instance dataset $\mcl{D}$ as a set of bags, 
    where each bag is a set of instances with a class label.
Each instance has $k$ attributes.
Therefore, each instance can be considered to be a vector in $\real^k$,
    assuming that each attribute is numeric.
Therefore, the set of all instances in the dataset, $\mcl{I}$,
    can be defined as a set of $k$-dimensional vectors.
That is, $ \mcl{I} \subseteq \real^k $.

Each bag in the dataset $\mcl{D}$ is 
    composed of a set of instances from $\mcl{I}$,
    along with a class label. % FOR SIMPLICITY, we ignore duplicate instances
Let $\mcl{C}$ be the set of all possible class labels.
Then we can define $\mcl{D}$,
    the multi-instance dataset, as 
    $\mcl{D} \subseteq ( \power (\mcl{I}) \times \mcl{C} )$,
    where $\times$ represents the cartesian product of the two sets.

The propositionalisation process can be viewed as a function
    mapping each bag $B_i \in \mcl{D}$ to 
    a single propositionalised instance $p_i \in ( \real^j \times \mcl{C} )$.
Note that $p_i$ has $j$ (non-class) attributes, 
    which need not be equal to $k$, the number of attributes of 
    each single instance in the original dataset.
Also note that the class value of $p_i$ will be 
    equal to that of $B_i$,
    i.e.\ the class value is unchanged by the propositionalisation process.
       

\subsection{Bisecting the instance space}

For the purposes of finding the best instance-space bisector,
    we first compute $\mcl{M}$, an intermediate dataset.
$\mcl{M}$ is built up by collecting together all single-instances
    from all the bags in the dataset and 
    attaching the class value of each bag to 
    each instance in the bag.
Formally, $\mcl{M} \subseteq (\mcl{I} \times \mcl{C})$, 
    where each instance of $\mcl{M}$ appears in $\mcl{D}$:
    $$
        \forall~(inst,c_i)\in\mcl{M} ~:~ 
            \exists~(bag,c_b) \in \mcl{D} ~:~ 
                inst \in bag ~\land~ c_i = c_b
    $$
    and each instance of $\mcl{D}$ appears in $\mcl{M}$:
    $$
        \forall~(bag,c_b) \in \mcl{D} ~:~ 
            \forall~inst \in bag ~:~
                (inst,c_b) \in \mcl{M}
    $$    

Then, this set of labelled instances, $\mcl{M}$, is used to partition the instance space by
    repeatedly bisecting the instance space
    to produce the regions.*
We restrict each bisecting hyperplane to be parallel to exactly one axis.
Therefore, we can represent each bisecting hyperplane as an attribute,
    paired with a split value).
    
[DIAGRAM, EXAMPLE]

\AdaProp selects bisecting hyperplanes in a greedy fashion. 
At each iteration, a set of candidate bisectors is generated.
Each candidate bisector is evaluated, 
    by propositionalisation and then using the single instance base learner
    and the bisector which results in the least training set error is chosen.
    
\subsubsection{Generating Candidate Bisectors}

Given a set of instances to bisect, 
    \AdaProp implements four methods for selecting candidate bisectors:
\begin{enumerate}
\item Range based midpoint ($\frac{\max+\min}{2}$) of an attribute
\item Mean of an attribute
\item Median of an attribute
\item Each class boundary of an attribute 
\end{enumerate}

Each of the first three methods generates exactly one candidate per attribute.
However, the class boundary based method, 
    which offers a candidate at each value where the class value changes,
    (similar to the discretization process of OneR~\cite{holte}), 
    may generate multiple candidates per attribute and 
    thus is more computationally expensive.
In order to prevent overfitting, 
    we restrict the boundaries offered by this method to have at least $minOcc$ instances
    in the training data on both sides of the boundary 
    (where $minOcc$ is a user parameter).

[PSEUDO-CODE]

\subsubsection{Selecting the best bisector}
Each candidate bisector was added to the current set of bisectors,
    and the entire MI dataset was propositionalised and then
    the base learner was trained on the propositionalised dataset
    and the training set error was recorded.
At each iteration, the bisector which results in the least training set error 
    is chosen to be added to the list of selected bisectors.

\subsubsection{Tree of Bisectors}

After deciding how to split once, we need to repeat this split to obtain a tree of partitions.
Originally, fill out the entire tree using depth first search.

Better approaches, i.e.\ Breadth first or Best-first (Heuristic) were tried as well. TODO

\subsection{Propositionalisation}
The tree of bisectors defines a set of regions in the instance space.
Each such region will correspond to one attribute in the propositionalised dataset.
Currently, \AdaProp propositionalises each bag 
    by simply counting the number of instances in the bag which fall into the region.
    
[Example, Formalize] 

In general, the attribute can be any summary statistic chosen by the user.
\AdaProp will eventually* implement support for aggregations such as 
    MIN, MAX, AVERAGE, SUM and STDDEV and 
    will thus allow the user to select any summary statistic 
    (or perhaps even a set of statistics*?) 
    to be used in the propositionalisation process.
    
[Final Prop Form?]
    

\subsection{The single-instance base learner}    
Any single instance base learner can be used to guide the bisector selection process.
The choice of base learner is left upto the user, and 
    this report considers a selection of common base learners (see results section).
*In this report, we consider: [LIST base learners from results ]    

*TODO* any properties satsified by the base learners?
-- MUST SUPPORT CLASS VALUES
    
\subsection{MISC}
This project will consider the adaptive partitioning approach to propositionalisation.
At each stage, a set of candidate partitions of the instance space will be generated. Each of these partitions will be evaluated using the  single-instance base learner and the best partition will be selected. This process will be repeated recursively for each partition resulting in a tree of regions. At the completion of this process, this tree will be used to propositionalise the MI bags in a way similar to that of \citeA{Weidmann2003}.


%Algorithm(s) will be implemented using the WEKA framework.*


\section{Results}
Experiments were conducted using 16 datasets from the UCI repository, 
    comparing the performance of three bisector generation methods 
    (range-based midpoint, mean, and median)
    and two base learners (j48 and SMO).
All experiments were conducted for the breadth*-first search strategy for 
    building up the tree of bisectors.
Experiments for the the fourth bisector generation method and best first search strategies 
    are ongoing.
    
First, we aggregate the results of all experiments and 
    compare the results for each bisector generation method:
    
[TODO]

Then compare the results across base learners 

[ TODO ]

Then we show the results over each dataset

[ Raw Data ? can be found at appendix ? ]    
    


\section{Results - Artificial dataset}
Since the results on the UCI datasets were unsatisfactory, 
    artificial datasets were generated to check that the 
    implementation of the \AdaProp algorithm.
    
Poor results with J48, overfitting the very simple dataset, better results with 
    OneR, more data and the last algorithm (what was it?)
Simpler base learner may work better in the simple datasets?

[ Learning curves ]


\section{Future Work}
Application towards image classification, consider representations
    convert existing SI image datasets to MI image datasets, etc

Nominal and missing attr

The final output of this project will be an algorithm which can be used to adaptively propositionalise multi-instance data. Since there are other established algorithms for handling multi-instance data, the evaluation will consist of comparing the classification accuracy of this algorithm on standard multi-instance datasets against the accuracy of methods such as MILES~\cite{Chen2006}, TLC~\cite{Weidmann2003} and DD~\cite{Maron98mil}.

Since this algorithm is a meta-learner, i.e.\ it is dependent on an underlying single-instance base learner, a number of different base learners will be used to evaluate the performance of this algorithm.
 
Finally, this algorithm will be evaluated on standard image classification datasets (for example the \citeA{sival}) and the performance will be compared to that of established image classification algorithms.


\section{Conclusion}
TODO

In this project, we will examine the propositionalisation of multi-instance data using a process which is influenced by the single-instance base learner being used, i.e. an adaptive propositionalisation process. The initial approach will be to partition the instance space by generating a set of candidate partitions and using the single instance learner to select the optimal partition. 
The result of this project is an algorithm, which will be evaluated by comparing its performance to that of existing multi-instance learning algorithms on standard MI datasets. 
In addition, the application of multi-instance learning for image classification will be examined and the algorithm will be tuned for dealing with image classification datasets.
\clearpage
\bibliographystyle{apacite} 
\bibliography{interim-report}
\end{document}
